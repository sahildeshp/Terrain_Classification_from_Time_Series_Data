{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectC1_DAug_and_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrfTvKPbgs13"
      },
      "source": [
        "###Importing all the neccessary libraries ###\n",
        "\n",
        "%tensorflow_version 1.x #Forcing the google collab to get tensorflow version 1.0\n",
        "import numpy as np # for mathematical operations with numpy matrix\n",
        "import pandas as pd #Data_Augmentation and getting the data\n",
        "import warnings\n",
        "\n",
        "from keras.utils import to_categorical #Convert to ylabels to one hot\n",
        "\n",
        "from sklearn.metrics import f1_score,classification_report #get f1 score and classification report\n",
        "import sklearn.model_selection as model_selection #For train_test Split for static training\n",
        "\n",
        "##Keras LIBS##\n",
        "from keras.models import Sequential,load_model \n",
        "from keras.layers import Dense,Flatten,Dropout,LSTM,BatchNormalization,TimeDistributed,Bidirectional,SimpleRNN,Conv1D,GRU,MaxPooling1D\n",
        "from keras.optimizers import  Adam\n",
        "##Keras LIBS##\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint #save the model for each checkpoint \n",
        "import matplotlib.pyplot as plt #for plots\n",
        "from google.colab import drive #need to mount drive use this only if you are using google collab to train  models\n",
        "from sklearn.preprocessing import StandardScaler #Not used here but can be used to get Gaussian/Normal Distribution of the data\n",
        "#TODO Find out why standard scalar is giving such a low F1 score on GradeScope\n",
        "\n",
        "\n",
        "###LIBRARY IMPORTING DONE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPxWMg_6iLE3"
      },
      "source": [
        "#Mount the respective drives to retrieve data stored in the google drive\n",
        "drive.mount('/content/drive')\n",
        "Root_Path=\"/content/drive/MyDrive/\" ##This is your root path :This is same for every google drive user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeL_OcMenSjV"
      },
      "source": [
        "###UTILITY class ###\n",
        "class utility():\n",
        "   \n",
        "    \"\"\"Func@1-create_time_windows\n",
        "    It creates segments of data for each timesteps\n",
        "    References:: book and blogs written By Jason Brownlee\n",
        "    @return X array with timetseps and Y array\"\"\"\n",
        "    def create_time_windows(self,sequence,n_steps):\n",
        "        X, y = [],[]\n",
        "\n",
        "        for i in range(len(sequence)):\n",
        "            # find the end of this pattern\n",
        "            end_ix = i + n_steps\n",
        "            if end_ix > len(sequence):\n",
        "                break\n",
        "                # gather input and output parts of the pattern\n",
        "            seq_x,seq_y = sequence[i:end_ix-1,:-1],sequence[end_ix-1,-1]###Data is different from the book\n",
        "\n",
        "            X.append(seq_x)\n",
        "            y.append(seq_y)\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    \"\"\"Func2-Creates optimised version of Time windows on stacked data\n",
        "    to prevent RAM overload\n",
        "    @return X array with timetseps and Y array \"\"\"\n",
        "\n",
        "    def create_time_windows_optfloat(self,sequence, n_steps):\n",
        "        X, y = [],[]\n",
        "        for i in range(len(sequence)):\n",
        "            # find the end of this pattern\n",
        "            end_ix = i + n_steps\n",
        "            if end_ix > len(sequence):\n",
        "                break\n",
        "                # gather input and output parts of the pattern\n",
        "            seq_x,seq_y = sequence[i:end_ix-1,:-1],sequence[end_ix-1,-1]###Data is different from the book\n",
        "\n",
        "            X.append(seq_x)\n",
        "            y.append(seq_y)\n",
        "        print(len(X))\n",
        "        X= np.asarray(X,dtype = np.float32)\n",
        "        y= np.asarray(y,dtype = np.float32)\n",
        "        return np.array(X),np.array(y)\n",
        "                                \n",
        "    \"\"\"Func3-shows plots for validation and loss\n",
        "    @return plots\"\"\"\n",
        "    def plot_history(self,history):\n",
        "        # plot loss\n",
        "        plt.title('Loss')\n",
        "        plt.plot(history.history['loss'], color='blue', label='train')\n",
        "        plt.plot(history.history['val_loss'], color='red', label='test')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'])\n",
        "        plt.show()\n",
        "        \n",
        "        # plot accuracy\n",
        "        plt.title('Accuracy')\n",
        "        plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "        plt.plot(history.history['val_accuracy'], color='red', label='test')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'])\n",
        "        plt.show()\n",
        "\n",
        "    def select_training_type(self):\n",
        "        training_type=0\n",
        "        print(\"Select the training/Testing type from the map\\n\")\n",
        "        print(\"For Static Training  A.1      press 1 \\n\")\n",
        "        print(\"For Static Training  A.2      press 2\\n\")\n",
        "        print(\"For Dynamic Training B        press 3\\n\")\n",
        "\n",
        "        print(\"After selecting the training type press enter\")\n",
        "        training_type=int(input(training_type))\n",
        "        return training_type\n",
        "###End of class Utility ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNjPNDNLEmDl"
      },
      "source": [
        "class Data_Augmentation():\n",
        "    \n",
        "    def type_1(self,Train_data_path):\n",
        "        print(\"You have selected type1 Data Augmentation \\n this type uses max of 3 datas from each subject and min of 3 datas from each subject which was atcked in a csv file\")\n",
        "        Train_Data_df=pd.read_csv(Train_data_path)\n",
        "        labels = Train_Data_df[\"class\"].to_numpy()\n",
        "        Ylabels = np.repeat(labels, 4)##Matching frequencies\n",
        "        Ylabels = Ylabels[~np.isnan(Ylabels)]\n",
        "        \"\"\"Just chug out the extra dimensions\"\"\"\n",
        "        Ylabels=np.delete(Ylabels,95735)\n",
        "        Ylabels=np.delete(Ylabels,95734)\n",
        "        Ylabels = Ylabels.reshape(Ylabels.shape[0], 1)\n",
        "\n",
        "        Train_Data_df[\"new_labels\"]=pd.DataFrame(Ylabels)\n",
        "        cols=[\"accx\",\"accy\",\"accz\",\"gyrx\",\"gyry\",\"gyrz\",\"new_labels\"]\n",
        "        new_Data=pd.DataFrame(Train_Data_df[cols])\n",
        "        print(new_Data.head(n=5))\n",
        "        #########PREPARING THE DATA X #############\n",
        "        trainX=new_Data.to_numpy()\n",
        "        return trainX,Ylabels\n",
        "\n",
        "    def type_2(self,Train_data_path):\n",
        "        print(\"You have selected type2 Data Augmentation \\n this type all the datas of 8 subjects are given for training\")\n",
        "        Train_Data_df = pd.read_csv(Train_data_path)\n",
        "        labels = Train_Data_df[\"class\"].to_numpy()\n",
        "        Ylabels = np.repeat(labels, 4)\n",
        "        Ylabels = Ylabels[~np.isnan(Ylabels)]\n",
        "        \"\"\"Just chug out the extra dimensions\"\"\"\n",
        "        Ylabels = np.delete(Ylabels, 1341650)\n",
        "        Ylabels = np.delete(Ylabels, 1341649)\n",
        "        Ylabels=np.delete(Ylabels,1341648)\n",
        "        Ylabels=np.delete(Ylabels,1341647)\n",
        "        Ylabels = np.delete(Ylabels, 1341646)\n",
        "        Ylabels = np.delete(Ylabels, 1341645)\n",
        "        Ylabels = Ylabels.reshape(Ylabels.shape[0], 1)\n",
        "\n",
        "        Train_Data_df[\"new_labels\"] = pd.DataFrame(Ylabels)\n",
        "        cols = [\"accx\", \"accy\", \"accz\", \"gyrx\", \"gyry\", \"gyrz\", \"new_labels\"]\n",
        "        new_Data = pd.DataFrame(Train_Data_df[cols])\n",
        "        print(new_Data.head(n=5))\n",
        "        #########PREPARING THE DATA #############\n",
        "        trainX = new_Data.to_numpy()\n",
        "        return trainX,Ylabels\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nhh_rcFALTh"
      },
      "source": [
        "####MODEL SELECTION CLASS ####\n",
        "\"\"\"Note:Hyperparameters are choosen through concventions of kagglers and blogs ,GridsearchCV,In team discussions,TA meetings and blogs\"\"\"\n",
        "class SelectModel():\n",
        "        \"\"\"Func@1 Creates an LSTM model with one DropOut layer\n",
        "            @return model\"\"\"\n",
        "        def __create_lstm_dropout_model(self,timesteps,features_acce_and_gyro,outputs):\n",
        "            model=Sequential()\n",
        "            model.add(LSTM(100,input_shape=(timesteps,features_acce_and_gyro)))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Dense(100,activation=\"relu\"))\n",
        "            model.add(Dense(outputs,activation=\"softmax\"))\n",
        "            model.summary()\n",
        "            return model\n",
        "        \"\"\"Func@2 Creates and LSTM model with some hyperparameter\n",
        "        tuning and adds a Batch_Normalisation layer to it\n",
        "        @return model\"\"\"\n",
        "        def __create_model_Batch_Norm(self,timesteps,features_acce_and_gyro,outputs):\n",
        "            model=Sequential()\n",
        "            model.add(LSTM(200,input_shape=(timesteps,features_acce_and_gyro)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dense(200,activation=\"relu\"))\n",
        "            model.add(Dense(outputs,activation=\"softmax\"))\n",
        "            model.summary()\n",
        "            return model\n",
        "        \"\"\"Func@3 Creates CNN_GRU model\n",
        "        @return model\"\"\" \n",
        "        def __model_CNN_GRU(self,timesteps,features_acce_and_gyro,outputs):\n",
        "            model = Sequential()\n",
        "            model.add(Conv1D(filters=64, kernel_size=7,input_shape=(timesteps,features_acce_and_gyro),activation='relu'))\n",
        "            model.add(MaxPooling1D(pool_size=4,padding='valid'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Conv1D(filters=64, kernel_size=7,activation='relu'))\n",
        "            model.add(MaxPooling1D(pool_size=2,padding='valid'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(GRU(\n",
        "            units=64,\n",
        "            activation='tanh',\n",
        "            recurrent_activation='sigmoid',\n",
        "            use_bias=True,\n",
        "            kernel_initializer='glorot_uniform',\n",
        "            return_sequences=True, \n",
        "            dropout=0.0,\n",
        "            recurrent_dropout=0.0, #0.458 \n",
        "            ))   \n",
        "            model.add(GRU(\n",
        "            units=64,\n",
        "            activation='tanh',\n",
        "            recurrent_activation='sigmoid',\n",
        "            use_bias=True,\n",
        "            kernel_initializer='glorot_uniform',\n",
        "            return_sequences=False, \n",
        "            recurrent_dropout=0.2 \n",
        "            ))\n",
        "            model.add(Dropout(0.1))\n",
        "            model.add(Dense(outputs, kernel_initializer='uniform',activation='softmax'))\n",
        "            model.summary()\n",
        "            return model\n",
        "\n",
        "        \"\"\"Func@4 Creates Simple_RNN_LSTM  model with dropout and dense and like Team 22\n",
        "        @return model\"\"\" \n",
        "        def __model_RNN_LSTM(self,timesteps,features_acce_and_gyro,outputs):\n",
        "            model=Sequential()\n",
        "            model.add(SimpleRNN(100,return_sequences=True,input_shape=(timesteps,features_acce_and_gyro)))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Dense(32,activation=\"relu\"))\n",
        "            model.add(LSTM(100,return_sequences=True))\n",
        "            model.add(Dense(32,activation='relu'))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(4,activation=\"softmax\"))\n",
        "\n",
        "            model.summary()\n",
        "            return model\n",
        "        \n",
        "        \"\"\"Func5 Creates a CNN1D only as used by Jason Brownlees\n",
        "            @return model\"\"\"\n",
        "        def __model_CNN1D(self,timesteps,features_acce_and_gyro,outputs):\n",
        "            model = Sequential()\n",
        "\n",
        "            # add model layers\n",
        "            model.add(Conv1D(filters=64, kernel_size=7,input_shape=(timesteps,features_acce_and_gyro),activation='relu'))\n",
        "            model.add(MaxPooling1D(pool_size=4,padding='valid'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Conv1D(filters=64, kernel_size=7,activation='relu'))\n",
        "            model.add(MaxPooling1D(pool_size=2,padding='valid'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(LSTM(100,return_sequences=False))  \n",
        "            model.add(Dropout(0.1))\n",
        "            model.add(Dense(outputs, kernel_initializer='uniform',activation='softmax'))\n",
        "            model.summary()\n",
        "            return model\n",
        "\n",
        "        \"\"\"Func5 Creates and LSTM model with some hyperparameter\n",
        "        tuning and adds a Batch_Normalisation layer to it\"\"\"\n",
        "        def __create_model_Batch_Norm_Bilstm(self,timesteps,features_acce_and_gyro,outputs):\n",
        "            model=Sequential()\n",
        "            model.add(Bidirectional(LSTM(200, return_sequences=True), input_shape=(timesteps,features_acce_and_gyro)))\n",
        "            model.add(LSTM(200,return_sequences=False))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dense(200,activation=\"relu\"))\n",
        "            model.add((Dense(outputs,activation=\"softmax\")))\n",
        "            model.summary()\n",
        "            return model\n",
        "        \n",
        "        \"\"\"Func6- Creates the model for dynamic training B this is only public function for \n",
        "        dynamic training with other model architectures make the other architectures public and call in if training_type=B in the main_function()\"\"\"\n",
        "        def shreedhar_LSTM2_model(self,n_timesteps,n_features,n_outputs):\n",
        "            model = Sequential()\n",
        "            model.add(LSTM(128, return_sequences=True , input_shape=(n_timesteps,n_features)))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(LSTM(128))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Dense(128, activation='relu'))\n",
        "            model.add(Dense(n_outputs, activation='softmax'))\n",
        "            model.summary()\n",
        "            return model\n",
        "\n",
        "\n",
        "        \"\"\"Func7-This is used for tscaked Training A1 and A2 where the user can choose the model archiecture they want to train on\n",
        "            return model\"\"\"\n",
        "        def Select_Model_for_Training(self,timesteps,features_acce_and_gyro,outputs):\n",
        "            model=0\n",
        "            print(\"From the map select which model do you want to use for training \\n\")\n",
        "            print(\"LSTM_WITH_SINGLE_LAYER_DROPOUT     press 1\\n\")\n",
        "            print(\"BATCH_NORM_SINGLE_LAYER_MODEL      press 2\\n\")\n",
        "            print(\"CNN_WITH_GRU                       press 3\\n\")\n",
        "            print(\"RNN_LSTM                           press 4\\n\")\n",
        "            print(\"CNN1D                              press 5\\n\")\n",
        "            print(\"BILSTM.                            press 6\\n\")\n",
        "            print(\"LSTM_2_Dropout                     press 7\\n\")\n",
        "            print(\"After Selecting press enter\")\n",
        "\n",
        "            model=int(input(model))\n",
        "            if(model==1):\n",
        "               print(\"selected LSTM_WITH_SINGLE_LAYER_DROPOUT here is the architecture\\n\") \n",
        "               model=self.__create_lstm_dropout_model(timesteps,features_acce_and_gyro,outputs)\n",
        "               return model,\"LSTM_DROPOUT\"\n",
        "            elif(model==2):\n",
        "                print(\"selected BATCH_NORM_SINGLE_LAYER_MODEL here is the architecture\\n\")\n",
        "                model=self.__create_model_Batch_Norm(timesteps,features_acce_and_gyro,outputs)\n",
        "                return model,\"BATCH_NORM_LSTM\"\n",
        "            elif(model==3):\n",
        "                print(\"selected CNN_WITH_GRU  model here is the architecture\\n\")\n",
        "                model=self.__model_CNN_GRU(timesteps,features_acce_and_gyro,outputs)\n",
        "                return model,\"CNN_GRU\"\n",
        "            elif(model==4):\n",
        "                print(\"selected RNN_LSTM_MODEL here is the architecture\\n\")\n",
        "                model=self.__model_RNN_LSTM(timesteps,features_acce_and_gyro,outputs)\n",
        "                return model,\"RNN_LSTM\"\n",
        "            elif(model==5):\n",
        "                print(\"selected CNN1D here is the architecture\\n\")\n",
        "                model=self.__model_CNN1D(timesteps,features_acce_and_gyro,outputs)\n",
        "                return model,\"CNN_1D\"\n",
        "            elif(model==6):\n",
        "                print(\"selected BILSTM here is the architecture\\n\")\n",
        "                model=self.__create_model_Batch_Norm_Bilstm(timesteps,features_acce_and_gyro,outputs)\n",
        "                return model,\"BILSTM\"\n",
        "            elif(model==7):\n",
        "                print(\"Selected Shreedhar model of LSTM_DROPout\")\n",
        "                model=self.shreedhar_LSTM2_model(timesteps,features_acce_and_gyro,outputs)\n",
        "                return model,\"SHREEDHAR_LSTM_DROPOUT\"\n",
        "            else:\n",
        "                print(\"Wrong option selected\\n\")\n",
        "                self.Select_Model_for_Training(timesteps,features_acce_and_gyro,outputs)\n",
        "####MODEL SELECTION CLASS ENDS ####    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmOKiDiRKuv"
      },
      "source": [
        "print(\"Keeping a check on all files in your Drive Project_C1\\n\")\n",
        "!ls \"/content/drive/My Drive/ECE542/Project_C1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPT_POJGaCtQ"
      },
      "source": [
        "We are testing out all different types of training and data augmentation done by different teams to give us the best type of results:\n",
        "\n",
        "    Now in Training stage \n",
        "    We will be having 2 main types of Training according to the Data   \n",
        "    Augmentation we have done:\n",
        "\n",
        "    (A) Training the model on static Data - Here the data of all subjects are stacked together in a single csv file and the frequency is resampled in the data augmentation class using np.repeat():\n",
        "\n",
        "         A.1-->Here there is a uniform distribution of data of subjects where max \n",
        "         3 datsets and min 1 dataset is taken from each subject,they were stacked \n",
        "        stacked together and resampled in Data_Augmentation.type1()\n",
        "\n",
        "            A.2-->Here all the subject datas were taken this is bruteforce training \n",
        "            forcing the model to memorise the data of all the\n",
        "            subjects:Data_Augmentation.type(2)  \n",
        "\n",
        "    (B) Training the model on dynamic data():\n",
        "        Here Data Augmentation is done for each subject indiviudally in a local\n",
        "        machine and model fitting is done on each subject individually\n",
        "\n",
        "At the end A.1 and B gave us good F1 scores of above 80 .We improved the data more by hand and went ahead with A.1 we didnt test the data training anymore with B.But you are most welcome to test it out and get results but A.2 just memorised the data and will give average F1 scores to be around 60-65,So it is better to avoid that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO4GOeS-mDD6"
      },
      "source": [
        "\"\"\"Run this cell multiple times for different models rest of the cells you can run once\"\"\"\n",
        "def main_func():\n",
        "    # #Use gridsearchCV here\n",
        "    # epochs=[10,15,20]      #10 is good enough -->15 is overfitting\n",
        "    # batch_size=[32,64,128] #32 makes it faster but 128 makes the gradient more uniform i.e slowly and surely it reaches global minima\n",
        "\n",
        "    \"\"\"Step 1: Set the paths according to your drive\"\"\"\n",
        "    print(\"\\n\")\n",
        "    Project_Files_path=Root_Path+\"ECE542/Project_C1\" #Change the string here towards your Project_Files_Path\n",
        "    checkpoint_model_will_be_at=Project_Files_path+ \"/Checkpoints/model_checkpoint.h5\" #Change towards your own model you ant to save at each step in the checkpoint folder\n",
        "    saved_weights_path=Project_Files_path+\"/SavedModels/\" #make a directory models in your drive and all the models after training will be saved here\n",
        "    #print to make sure\n",
        "    print(\"Your Project Filies Path is given as \"+ Project_Files_path+\"\\n\"+\"The Checkpoints model will be saved at \"+ checkpoint_model_will_be_at +\"\\n\" + \"The model after training will be saved at \"+saved_weights_path +\"\\n\")\n",
        "    #print a WARNING\n",
        "    warnings.warn(\"Please check if all the paths are correctly otherwise your model will not be saved at all and you may have to retrain it\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    \"\"\"Step2: Create class objects\"\"\"\n",
        "    Utility=utility()##Create utility class object to be used later keep it as singleton object\n",
        "    model=SelectModel() ##Create model class object to be used later keep it as a singleton object\n",
        "    Augment=Data_Augmentation() ##The type of data augmentation you want\n",
        "\n",
        "    \"\"\"Step 3: Go for Training\"\"\"\n",
        "    training_type=Utility.select_training_type()\n",
        "\n",
        "   ###TRAINING TYPE A.1 Starts ####\n",
        "\n",
        "    if(training_type == 1):\n",
        "        model_is_saved=False ######CHANGE THIS TO FALSE IF YOU WANT TO RETRAIN THE MODEL####\n",
        "        if(model_is_saved==False):\n",
        "            print(\"Selected Training A.1\")\n",
        "            Train_data_path=Project_Files_path+\"/xy.csv\"\n",
        "            trainX,trainY=Augment.type_1(Train_data_path)\n",
        "            X_New,Y_New=Utility.create_time_windows_optfloat(trainX,100) #feature sets has to be optimised according to RAM left and data type \n",
        "            #Convert the Ynew into one-hot labels\n",
        "            Y_labels_one_hot_encoded=to_categorical(Y_New,num_classes=4)\n",
        "\n",
        "            ##Split train and validation sets\n",
        "            ##testsets will be given prepared differently so no need to slice the array into 3 parts\n",
        "            Xtrain,XVal,Ytrain,YVal=model_selection.train_test_split(X_New,Y_labels_one_hot_encoded,train_size=0.8,test_size=0.2)\n",
        "            timesteps,features,outputs=Xtrain.shape[1],Xtrain.shape[2],Ytrain.shape[1]\n",
        "            ###TO STOP RAM GOIUNG OVERBOARD###\n",
        "            del X_New\n",
        "            del Y_New\n",
        "            del Y_labels_one_hot_encoded\n",
        "            ###TO STOP RAM GOIUNG OVERBOARD###\n",
        "            model,label=model.Select_Model_for_Training(timesteps,features,outputs)\n",
        "            checkpoint = ModelCheckpoint(checkpoint_model_will_be_at, monitor='loss', verbose=1,\n",
        "            save_best_only=True, mode='auto', period=1)\n",
        "            model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])\n",
        "            history=model.fit(Xtrain,Ytrain,epochs=10,batch_size=128,validation_data=(XVal,YVal),verbose=1,callbacks=[checkpoint])\n",
        "            Utility.plot_history(history)\n",
        "            \n",
        "            saved_weights_path=saved_weights_path+label+\"_typeA.1\"+\".h5\"\n",
        "            print(\"The model is saved at\"+saved_weights_path)\n",
        "            model.save(saved_weights_path)\n",
        "            del model\n",
        "            \n",
        "        else:\n",
        "            Val_data_path=Project_Files_path+\"/xy.csv\"\n",
        "            trainX,trainY=Augment.type_1(Val_data_path)\n",
        "            X_New,Y_New=Utility.create_time_windows_optfloat(trainX,100) #feature sets has to be optimised according to RAM left and data type \n",
        "            Xtrain,XVal,Ytrain,YVal=model_selection.train_test_split(X_New,Y_New,train_size=0.8,test_size=0.2)\n",
        "            model=load_model(\"/content/drive/MyDrive/ECE542/Project_C1/SavedModels/CNN_GRU_typeA.1.h5\")\n",
        "            ypred=model.predict(XVal,verbose=1)\n",
        "            ypred=np.argmax(ypred, axis=1)\n",
        "            print(ypred)\n",
        "            yactual=YVal\n",
        "            target_names = ['class 0', 'class 1', 'class 2',\"class 3\"]\n",
        "            print(classification_report(yactual, ypred,target_names=target_names))\n",
        "            del model #free the RAM\n",
        "\n",
        "        ###TRAINING TYPE A.1 ENDS ####\n",
        "\n",
        "   \n",
        "    ###TRAINING TYPE A.2 STARTS ####\n",
        "    elif(training_type ==2):\n",
        "        if(model_is_saved==False):\n",
        "            print(\"Selected Training A.2\")\n",
        "            Train_data_path=Project_Files_path+\"/final_train_xy.csv\"\n",
        "            trainX,trainY=Augment.type_1(Train_data_path)\n",
        "            X_New,Y_New=Utility.create_time_windows_optfloat(trainX,100) #feature sets has to be optimised according to RAM left and data type \n",
        "            #Convert the Ynew into one-hot labels\n",
        "            Y_labels_one_hot_encoded=to_categorical(Y_New,num_classes=4)\n",
        "\n",
        "            ##Split train and validation sets\n",
        "            ##testsets will be given prepared differently so no need to slice the array into 3 parts\n",
        "            Xtrain,XVal,Ytrain,YVal=model_selection.train_test_split(X_New,Y_labels_one_hot_encoded,train_size=0.8,test_size=0.2)\n",
        "            timesteps,features,outputs=Xtrain.shape[1],Xtrain.shape[2],Ytrain.shape[1]\n",
        "            ###TO STOP RAM GOIUNG OVERBOARD###\n",
        "            del X_New\n",
        "            del Y_New\n",
        "            del Y_labels_one_hot_encoded\n",
        "            ###TO STOP RAM GOIUNG OVERBOARD###\n",
        "            model,label=model.Select_Model_for_Training(timesteps,features,outputs)\n",
        "            checkpoint = ModelCheckpoint(checkpoint_model_will_be_at, monitor='loss', verbose=1,\n",
        "            save_best_only=True, mode='auto', period=1)\n",
        "            model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])\n",
        "            history=model.fit(Xtrain,Ytrain,epochs=10,batch_size=128,validation_data=(XVal,YVal),verbose=1,callbacks=[checkpoint])\n",
        "            Utility.plot_history(history)\n",
        "\n",
        "            saved_weights_path=saved_weights_path+label+\"_typeA.2\"+\".h5\"\n",
        "            model.save(saved_weights_path)\n",
        "            print(\"The model is saved at\"+saved_weights_path)\n",
        "            del model\n",
        "            \n",
        "        else:\n",
        "            Val_data_path=Project_Files_path+\"/final_train_xy.csv\"\n",
        "            trainX,trainY=Augment.type_1(Val_data_path)\n",
        "            X_New,Y_New=Utility.create_time_windows_optfloat(trainX,100) #feature sets has to be optimised according to RAM left and data type \n",
        "            Xtrain,XVal,Ytrain,YVal=model_selection.train_test_split(X_New,Y_New,train_size=0.8,test_size=0.2)\n",
        "\n",
        "            model=load_model(\"/content/drive/My Drive/ECE542/Project_C1/model.h5\") ###YOU HAVE GIVE YOUR PATH TO THE MODEL SAVD NOW\n",
        "            ypred=model.predict(XVal,verbose=1)\n",
        "            ypred=np.argmax(ypred, axis=1)\n",
        "            print(ypred)\n",
        "            yactual=YVal\n",
        "            target_names = ['class 0', 'class 1', 'class 2',\"class 3\"]\n",
        "            print(classification_report(yactual, ypred,target_names=target_names))  \n",
        "            del model \n",
        "        ###TRAINING TYPE A.2 ENDS ####\n",
        "      \n",
        "    ###TRAINING TYPE B STARTS ####\n",
        "    elif(training_type==3):\n",
        "        Acc=list()\n",
        "        print(\"Training is done dynamicaly: B\")\n",
        "        df = pd.read_csv('/content/drive/My Drive/merged/subject_008_01__xy.csv')\n",
        "        df.columns = ['0', '1', '2', '3', '4', '5', 'time', 'y', 'ytime', 'yactual']\n",
        "        columns = ['0', '1', '2', '3', '4', '5', 'y']\n",
        "        df = df[columns]\n",
        "        Xtest, ytest = Utility.create_time_windows_optfloat(df.values, 100)\n",
        "        Ytest = to_categorical(ytest, num_classes=4)\n",
        "        samples = [0, 8, 5, 3, 2, 3, 3, 4, 1]\n",
        "        for i in range(1, 8):\n",
        "            for j in range(1, samples[i]+1):\n",
        "                savepath = '/content/drive/My Drive/merged/'+'subject_00'+str(i)+'_0' + str(j) + '__xy.csv'\n",
        "                \n",
        "                df = pd.read_csv(savepath)\n",
        "                df.columns = ['0', '1', '2', '3', '4', '5', 'time', 'y', 'ytime', 'yactual']\n",
        "                columns = ['0', '1', '2', '3', '4', '5', 'y']\n",
        "                df = df[columns]\n",
        "                Xtrain,Ytrain=Utility.create_time_windows_optfloat(df.values,100) #feature sets has to be optimised according to RAM left and data type \n",
        "                #Convert the Ytrain into one-hot labels\n",
        "                Ytrain=to_categorical(Ytrain,num_classes=4)\n",
        "                print(\"Training model on \"+ ' subject_00 '+str(i)+'_0' + str(j))\n",
        "                \n",
        "                timesteps,features,outputs=Xtrain.shape[1],Xtrain.shape[2],Ytrain.shape[1]\n",
        "                model=model.shreedhar_LSTM2_model(timesteps,features,outputs)\n",
        "                checkpoint = ModelCheckpoint(checkpoint_model_will_be_at, monitor='loss', verbose=1,\n",
        "                save_best_only=True, mode='auto', period=1)\n",
        "                model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])\n",
        "                history=model.fit(Xtrain,Ytrain,epochs=10,batch_size=32,verbose=1,callbacks=[checkpoint])\n",
        "\n",
        "                _, accuracy = model.evaluate(Xtest, Ytest, batch_size=batch_size, verbose=1)\n",
        "                print(\"On\"+' subject_00 '+str(i)+'_0' + str(j)+\"Achieved accuracy\"+accuracy)\n",
        "                Acc.append(accuracy)\n",
        "            \n",
        "        \n",
        "        saved_weights_path=saved_weights_path+label+\".h5\"\n",
        "        model.save(saved_weights_path)\n",
        "        print(\"Accuracy of the model\"+(sum(Acc)/len(Acc)))\n",
        "        del model\n",
        "        ###TRAINING TYPE B ENDS ####\n",
        "\n",
        "    else:\n",
        "        print(\"Wrong input select again\")\n",
        "        training_type=0;\n",
        "        main_func()  \n",
        "\n",
        "\n",
        "###CALL THE MAIN FUNCTION HERE###\n",
        "main_func()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUBA502qd5Ho"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}